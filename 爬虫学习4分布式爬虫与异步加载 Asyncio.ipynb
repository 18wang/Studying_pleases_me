{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式爬虫<br>\n",
    "让爬虫更快更有效率的方法,就是采用分布式.爬虫要花大量的时间在下载网页上.利用分布式可以实现在下载网页的**同时解析**另一部分网页.<br>\n",
    "下面是一个爬取[莫烦python](https://morvanzhou.github.io/)主页网站的程序(!网站的下载速度受网速影响),没有特定的任务,第一次使用普通方法爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备工作以及两个任务\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen,urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "base_url='https://morvanzhou.github.io/'\n",
    "#因为服务器的限制,发出的请求必须是很有限的\n",
    "restricted_crawl=True\n",
    "\n",
    "#定义爬取网页任务\n",
    "def crawl(url):\n",
    "    back=urlopen(url)\n",
    "    return back.read().decode()\n",
    "#定义网页查找任务\n",
    "def parse(html):\n",
    "    soup=BeautifulSoup(html,'lxml')\n",
    "    urls = soup.find_all('a',href=re.compile('^/.+?/$'))\n",
    "    title=soup.find('h1').get_text().strip()\n",
    "    #set()可以实现对括号内容去重复功能,输出正向排序\n",
    "    page_urls=set(urljoin(base_url,u['href']) for u in urls)\n",
    "    url=soup.find('meta',property='og:url')['content']\n",
    "    return title,page_urls,url\n",
    "\n",
    "#一次常规爬取,没有多核运算\n",
    "#设置已爬取的和未爬取的,避免重复\n",
    "unseen=set([base_url,])\n",
    "seen=set()\n",
    "\n",
    "#网站个数和计时\n",
    "count=0\n",
    "start=time.time()\n",
    "#仍然有网站时爬取\n",
    "while len(unseen)!=0:\n",
    "    #爬取20个网站则终止\n",
    "    if restricted_crawl and len(seen)>20:\n",
    "        break\n",
    "    #htmls存放爬取内容\n",
    "    htmls=[crawl(url) for url in unseen]\n",
    "    #results存放每个html对应的解析内容\n",
    "    results=[parse(h) for h in htmls]\n",
    "    #将unseen中的网址添加到seen中去\n",
    "    seen.update(unseen)\n",
    "    #unseen清空\n",
    "    unseen.clear()\n",
    "    for title,page_urls,url in results:\n",
    "        count+=1\n",
    "        print(count,title,url)\n",
    "        unseen.update(page_urls-seen)\n",
    "print('total time:%.1f s' %(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**好像把两段程序放在一个cell里才可以正确运行,因为某些变量not callalbe**<br>\n",
    "现在让我们来对比一下分布式爬取的效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备工作以及两个任务\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen,urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "base_url='https://morvanzhou.github.io/'\n",
    "#因为服务器的限制,发出的请求必须是很有限的\n",
    "restricted_crawl=True\n",
    "\n",
    "#定义爬取网页任务\n",
    "def crawl(url):\n",
    "    back=urlopen(url)\n",
    "    return back.read().decode()\n",
    "#定义网页查找任务\n",
    "def parse(html):\n",
    "    soup=BeautifulSoup(html,'lxml')\n",
    "    urls = soup.find_all('a',href=re.compile('^/.+?/$'))\n",
    "    title=soup.find('h1').get_text().strip()\n",
    "    #set()可以实现对括号内容去重复功能,输出正向排序\n",
    "    page_urls=set(urljoin(base_url,u['href']) for u in urls)\n",
    "    url=soup.find('meta',property='og:url')['content']\n",
    "    return title,page_urls,url\n",
    "#设置已爬取的和未爬取的,避免重复\n",
    "unseen=set([base_url,])\n",
    "seen=set()\n",
    "\n",
    "#用分布式 pool 操作,4核\n",
    "pool=mp.Pool(4)\n",
    "count,start=0,time.time()\n",
    "while len(unseen)!=0:\n",
    "    if restricted_crawl and len(seen)>20:\n",
    "        break\n",
    "    htmls=[pool.apply_async(crawl,args=(u,)).get() for u in unseen]\n",
    "    results=[pool.apply_async(prase,args=(h,)).get() for h in htmls]\n",
    "    seen.update(unseen)\n",
    "    unseen.clear()\n",
    "    for title,page_urls,url in results:\n",
    "        count+=1\n",
    "        print(count,tltle,url)\n",
    "        unseen.update(page_urls-seen)\n",
    "print('total time is %.2f s'%(time.time()-start))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异步加载asyncio<br>\n",
    "异步加载能够利用单线程实现多线程操作模式,示意图如下:\n",
    "<img src=\"https://morvanzhou.github.io/static/results/scraping/4-2-1.png\" width=\"500\" height=\"500\" />\n",
    "asyncio 不是多进程, 也不是多线程, 单单是一个线程, 但是是在 Python 的功能间切换着执行. 切换的点用 await 来标记, 能够异步的功能用 async 标记, 比如 async def function():<br>\n",
    "写程序比较asyncio与顺序执行的不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#普通程序的顺序执行\n",
    "import time\n",
    "def job(t):\n",
    "    print('start job:',t)\n",
    "    time.sleep(t)\n",
    "    print('job ',t,' takes ',t,'s.')\n",
    "\n",
    "def main():\n",
    "    [job(i) for i in range(1,3)] #执行1s和2s任务\n",
    "    \n",
    "start=time.time()\n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "print('total time is ',time.time()-start,'s')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def job(t):\n",
    "    print('start job ',t)\n",
    "    await asyncio.sleep(t)\n",
    "    print('job ',t,' takes ',t,'s')\n",
    "\n",
    "async def main(loop):\n",
    "    tasks=[loop.create_task(job(i)) for i in range(1,3)]\n",
    "    await asyncio.wait(tasks)\n",
    "    \n",
    "start=time.time()\n",
    "if __name__=='__main__':\n",
    "    loop=asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main(loop))\n",
    "print('using asyncio total time is ',time.time()-start,'s')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与aiohttp合用<br>\n",
    "上边的asyncio的性质可以说非常适合爬虫了.**aiohttp**是一个配合asyncio使用的工具,有爬取网站的功能."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/']\n",
      "open morvanpage 2times total time: 0.9378917217254639\n"
     ]
    }
   ],
   "source": [
    "#我抄的,没搞熟\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "async def job(session):\n",
    "    response=await session.get(URL) #爬取session相应的网站\n",
    "    return str(response.url)  #传回session网站\n",
    "\n",
    "async def main(loop):\n",
    "    async with aiohttp.ClientSession() as session:  #创建一个session,推荐使用session\n",
    "        tasks=[loop.create_task(job(session)) for _ in range(2)] #创建任务\n",
    "        finished,unfinished=await asyncio.wait(tasks)  #读取任务情况,并输出\n",
    "        print([f.result() for f in finished])\n",
    "\n",
    "start=time.time()\n",
    "if __name__=='__main__':\n",
    "    URL = 'https://morvanzhou.github.io/'\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main(loop))\n",
    "print('open morvanpage 2times total time:',time.time()-start)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asyncio与multiprocessing合用<br>\n",
    "<img src='https://morvanzhou.github.io/static/results/scraping/4-2-3.png' height=700 width=500>\n",
    "两者的优势互补,结果会更强."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
